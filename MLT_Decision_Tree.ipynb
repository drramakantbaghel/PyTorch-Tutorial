{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLT Decision Tree.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDTFyieOBxBOvpZz6p/UJs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drramakantbaghel/PyTorch-Tutorial/blob/master/MLT_Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWi0vneQSUZl"
      },
      "source": [
        "References:\n",
        "\n",
        "1. https://www.analyticssteps.com/blogs/introduction-decision-tree-algorithm-machine-learning\n",
        "\n",
        "2. https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm\n",
        "\n",
        "3. https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
        "\n",
        "4. https://www.geeksforgeeks.org/decision-tree-introduction-example/\n",
        "\n",
        "5. https://en.wikipedia.org/wiki/Decision_tree_learning\n",
        "\n",
        "6. https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
        "\n",
        "7. https://scikit-learn.org/stable/modules/tree.html\n",
        "\n",
        "8. https://medium.com/datadriveninvestor/easy-implementation-of-decision-tree-with-python-numpy-9ec64f05f8ae\n",
        "\n",
        "9. https://www.geeksforgeeks.org/decision-tree-implementation-python/\n",
        "\n",
        "10. https://www.python-course.eu/Decision_Trees.php\n",
        "\n",
        "11. https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2pVH-gsnq_V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mwnBrSmWCGZ"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NFum2ASNwr5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9QbDYgBPoqI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpDoXAd5n4-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRdZjnTvO5hZ"
      },
      "source": [
        "**What is a Decision Tree?**\n",
        " \n",
        "\n",
        "\n",
        "* A Supervised learning technique \n",
        ">* For both classification and Regression problems\n",
        ">* Preferred for solving Classification problems. \n",
        "*tree-structured classifier \n",
        " > internal nodes - features of a dataset\n",
        "\n",
        " > branches -decision rules \n",
        "\n",
        " > each leaf node - outcome.\n",
        "\n",
        "\n",
        "\n",
        "*Two types of nodes, >the Decision Node and Leaf Node. \n",
        ">Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.\n",
        "\n",
        "\n",
        ">The decisions or the test are performed on the basis of features of the given dataset.\n",
        "\n",
        "> graphical representation for getting all the possible solutions to a problem/decision based on given conditions.\n",
        "\n",
        "\n",
        ">In order to build a tree, we use the CART algorithm, which stands for Classification and Regression Tree algorithm.\n",
        "\n",
        "* A decision tree simply asks a question\n",
        "> based on the answer (Yes/No)\n",
        "\n",
        "> it further split the tree into subtrees\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIUF5vasUHrR"
      },
      "source": [
        "* **Branches** - Division of the whole tree is called branches.\n",
        "\n",
        "* **Root Node** - Represent the whole sample that is further divided.\n",
        "\n",
        "* **Splitting** - Division of nodes is called splitting.\n",
        "\n",
        "* **Terminal Node** - Node that does not split further is called a terminal node.\n",
        "\n",
        "* **Decision Node**  -  It is a node that also gets further divided into different sub-nodes being a sub node. \n",
        "\n",
        "* **Pruning** - Removal of subnodes from a decision node.\n",
        "\n",
        "* **Parent and Child Node** - When a node gets divided further then that node is termed as parent node whereas the divided nodes or the sub-nodes are termed as a child node of the parent node.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSivipd9UrM3"
      },
      "source": [
        "**Types of Decision Tree**\n",
        " > type of input we have that is categorical or numerical \n",
        "\n",
        ">* Categorical variable decision tree. \n",
        "\n",
        ">>categorical variable like whether the loan contender will defaulter or not, that is either yes/no.\n",
        "\n",
        ">* Continuous variable decision tree.\n",
        ">>numeric types and or is continuous in nature like when we have to predict a house price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwGWG1sYVfJp"
      },
      "source": [
        "**Why use Decision Trees?** \n",
        "\n",
        ">*Decision Trees usually mimic human thinking ability while making a decision, so it is easy to understand.\n",
        "\n",
        ">*The logic behind the decision tree can be easily understood because it shows a tree-like structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXEUgBB7WLXF"
      },
      "source": [
        "**Advantages**\n",
        " \n",
        "\n",
        "* effective and very simple.\n",
        "\n",
        "* Used for dealing with the missing values in the dataset.\n",
        "\n",
        "* For both numeric as well as categorical features.\n",
        "\n",
        "* For Inferences or results does not require any statistical or mathematics knowledge to be explained.\n",
        "\n",
        "* There is less requirement of data cleaning compared to other algorithms.\n",
        " \n",
        "\n",
        "**Disadvantages**\n",
        " \n",
        "\n",
        "* Logics get transformed if there are even small changes in training data.\n",
        "\n",
        "* Larger trees get difficult to interpret.\n",
        "\n",
        "* Biased towards three having more levels.\n",
        "\n",
        "* It may have an overfitting issue, which can be resolved using the Random Forest algorithm.\n",
        "\n",
        "* For more class labels, the computational complexity of the decision tree may increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucjAhgDsYsLS"
      },
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "* Image from Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFI8P2RlZN_n"
      },
      "source": [
        "**Types of Algorithms**\n",
        "\n",
        "* **ID3 (Iterative Dicotomizer3)**– By Ross Quinlan that uses greedy algorithms to generate multiple branch trees. Trees extend to maximum size before pruning.\n",
        "\n",
        "* **C4.5**  \n",
        " > flourished ID3 by overcoming restrictions of features that are required to be categorical. It effectively defines distinct attributes for numerical features. Using if-then condition it converts the trained trees. \n",
        "\n",
        "* **C5.0** uses less space and creates smaller rulesets than C4.5.\n",
        "\n",
        "* **The CART** classification and regression tree are similar to C4.5 but it braces numerical target variables and does not calculate the rule sets. It generates a binary tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfDHG_ivaYXW"
      },
      "source": [
        "**How does the Decision Tree algorithm Work?**\n",
        "\n",
        "\n",
        "* **Step-1:** Begin the tree with the root node, says S, which contains the complete dataset.\n",
        "* **Step-2:** Find the best attribute in the dataset using Attribute Selection Measure (ASM).\n",
        "* **Step-3:** Divide the S into subsets that contains possible values for the best attributes.\n",
        "* **Step-4:** Generate the decision tree node, which contains the best attribute.\n",
        "* **Step-5:** Recursively make new decision trees using the subsets of the dataset created in step -3.\n",
        ">  Continue this process until a stage is reached where you cannot further classify the nodes and called the final node as a leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2IxzSpbHP1"
      },
      "source": [
        "**Attribute Selection Measures(ASM)**\n",
        "\n",
        "*  how to select the best attribute for the root node and for sub-nodes. \n",
        "\n",
        "* There are two popular techniques for ASM, which are:\n",
        "1. **Information Gain:**\n",
        " > Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute.\n",
        "\n",
        " > It calculates how much information a feature provides us about a class.\n",
        "According to the value of information gain, we split the node and build the decision tree.\n",
        "\n",
        "> **A decision tree algorithm always tries to maximize the value of information gain, and a node/attribute having the highest information gain is split first.** \n",
        "\n",
        "* **Information Gain= Entropy(S)-[(Weighted Avg) *Entropy(each feature)]** \n",
        "\n",
        "**Entropy: **\n",
        "* Entropy is a metric to measure the impurity in a given attribute. It specifies randomness in data. Entropy can be calculated as:\n",
        "\n",
        "**Entropy(s)= -P(yes)log2 P(yes)- P(no) log2 P(no)**\n",
        "Where,  \n",
        "\n",
        " * >  S= Total number of samples, \n",
        " * >   P(yes)= probability of yes\n",
        " * > P(no)= probability of no\n",
        "\n",
        "\n",
        "\n",
        "2. **Gini Index:**\n",
        "\n",
        " > Gini index is a measure of impurity or purity used while creating a decision tree in the CART(Classification and Regression Tree) algorithm.\n",
        "\n",
        "\n",
        "*An attribute with the low Gini index should be preferred as compared to the high Gini index.\n",
        "\n",
        "\n",
        "**It only creates binary splits, and the CART algorithm uses the Gini index to create binary splits.**\n",
        "\n",
        "\n",
        "* >  Gini Index= 1- ∑jPj2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Pruning: **  \n",
        "\n",
        "* Getting an Optimal Decision tree\n",
        "\n",
        "* Pruning is a process of deleting the unnecessary nodes from a tree in order to get the optimal decision tree.\n",
        "\n",
        "**A too-large tree increases the risk of overfitting, and a small tree may not capture all the important features of the dataset.**\n",
        "\n",
        " Therefore, a technique that decreases the size of the learning tree without reducing accuracy is known as Pruning. \n",
        " \n",
        "** There are mainly two types of tree pruning technology used:**\n",
        "\n",
        "Cost Complexity Pruning\n",
        "Reduced Error Pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6JFXPJ0deaj"
      },
      "source": [
        "**Steps for Python Implementation: **\n",
        "\n",
        "* Data Pre-processing step\n",
        "* Fitting a Decision-Tree algorithm to the Training set\n",
        "* Predicting the test result\n",
        "* Test accuracy of the result(Creation of Confusion matrix)\n",
        "* Visualizing the test set result.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aJMv1tojyVi"
      },
      "source": [
        "https://www.python-course.eu/Decision_Trees.php\n",
        "\n",
        "\n",
        "In this  example the animals are classified as being Mammals or Reptiles based on whether they are toothed, have legs and do breath. The dataset looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGl5Gj1JjwR9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\n",
        "                     \"hair\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"False\"],\n",
        "                     \"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n",
        "                     \"legs\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"True\"],\n",
        "                     \"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]}, \n",
        "                    columns=[\"toothed\",\"hair\",\"breathes\",\"legs\",\"species\"])\n",
        "\n",
        "features = data[[\"toothed\",\"hair\",\"breathes\",\"legs\"]]\n",
        "target = data[\"species\"]\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgyt_Qdtn6ov"
      },
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3owqpmQoBJV"
      },
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# test Gini values\n",
        "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
        "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxit1UO-oHD5"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbFFm_FtoMZU"
      },
      "source": [
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdogqYxJraVp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOYXff3vrhl_"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "dataset = [[2.771244718,1.784783929,0],\n",
        "\t[1.728571309,1.169761413,0],\n",
        "\t[3.678319846,2.81281357,0],\n",
        "\t[3.961043357,2.61995032,0],\n",
        "\t[2.999208922,2.209014212,0],\n",
        "\t[7.497545867,3.162953546,1],\n",
        "\t[9.00220326,3.339047188,1],\n",
        "\t[7.444542326,0.476683375,1],\n",
        "\t[10.12493903,3.234550982,1],\n",
        "\t[6.642287351,3.319983761,1]]\n",
        "split = get_split(dataset)\n",
        "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ypkp4Gpr958"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0rHzlOfokVW"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "dataset = [[2.771244718,1.784783929,0],\n",
        "\t[1.728571309,1.169761413,0],\n",
        "\t[3.678319846,2.81281357,0],\n",
        "\t[3.961043357,2.61995032,0],\n",
        "\t[2.999208922,2.209014212,0],\n",
        "\t[7.497545867,3.162953546,1],\n",
        "\t[9.00220326,3.339047188,1],\n",
        "\t[7.444542326,0.476683375,1],\n",
        "\t[10.12493903,3.234550982,1],\n",
        "\t[6.642287351,3.319983761,1]]\n",
        "split = get_split(dataset)\n",
        "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNa6ytB9o1kc"
      },
      "source": [
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYetCOzo_3O"
      },
      "source": [
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlwwqW3EpGfd"
      },
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6PBY8GUpTfA"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root\n",
        "\n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "\tif isinstance(node, dict):\n",
        "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "\t\tprint_tree(node['left'], depth+1)\n",
        "\t\tprint_tree(node['right'], depth+1)\n",
        "\telse:\n",
        "\t\tprint('%s[%s]' % ((depth*' ', node)))\n",
        "\n",
        "dataset = [[2.771244718,1.784783929,0],\n",
        "\t[1.728571309,1.169761413,0],\n",
        "\t[3.678319846,2.81281357,0],\n",
        "\t[3.961043357,2.61995032,0],\n",
        "\t[2.999208922,2.209014212,0],\n",
        "\t[7.497545867,3.162953546,1],\n",
        "\t[9.00220326,3.339047188,1],\n",
        "\t[7.444542326,0.476683375,1],\n",
        "\t[10.12493903,3.234550982,1],\n",
        "\t[6.642287351,3.319983761,1]]\n",
        "tree = build_tree(dataset, 1, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP602PBxpzCE"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root\n",
        "\n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "\tif isinstance(node, dict):\n",
        "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "\t\tprint_tree(node['left'], depth+1)\n",
        "\t\tprint_tree(node['right'], depth+1)\n",
        "\telse:\n",
        "\t\tprint('%s[%s]' % ((depth*' ', node)))\n",
        "\n",
        "dataset = [[2.771244718,1.784783929,0],\n",
        "\t[1.728571309,1.169761413,0],\n",
        "\t[3.678319846,2.81281357,0],\n",
        "\t[3.961043357,2.61995032,0],\n",
        "\t[2.999208922,2.209014212,0],\n",
        "\t[7.497545867,3.162953546,1],\n",
        "\t[9.00220326,3.339047188,1],\n",
        "\t[7.444542326,0.476683375,1],\n",
        "\t[10.12493903,3.234550982,1],\n",
        "\t[6.642287351,3.319983761,1]]\n",
        "tree = build_tree(dataset, 3, 1)\n",
        "print_tree(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHrrcb9HqMaZ"
      },
      "source": [
        "Another **Example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k44RAzAqQ_B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN-Hp_zPqYBD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\n",
        "                     \"hair\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"False\"],\n",
        "                     \"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n",
        "                     \"legs\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"True\"],\n",
        "                     \"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]}, \n",
        "                    columns=[\"toothed\",\"hair\",\"breathes\",\"legs\",\"species\"])\n",
        "\n",
        "features = data[[\"toothed\",\"hair\",\"breathes\",\"legs\"]]\n",
        "target = data[\"species\"]\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZMw36hor_66"
      },
      "source": [
        "# CART on the Bank Note dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tfile = open(filename, \"rt\")\n",
        "\tlines = reader(file)\n",
        "\tdataset = list(lines)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "\n",
        "# Classification and Regression Tree Algorithm\n",
        "def decision_tree(train, test, max_depth, min_size):\n",
        "\ttree = build_tree(train, max_depth, min_size)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(tree, row)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)\n",
        "\n",
        "# Test CART on Bank Note dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'data_banknote_authentication.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(len(dataset[0])):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}